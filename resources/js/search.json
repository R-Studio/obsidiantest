[[{"l":"obsidiantest","p":["This is a repostitory to test the Obsidian Markdown editor and also the GitHub Community Plugin to automatically commit & push changes to the GitHub repo.","More information's about the GitHub sync visit: https://desktopofsamuel.com/how-to-sync-obsidian-vault-for-free-using-git/"]},{"l":"Some Test Text","p":["Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsugfgm dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."]},{"l":"Subtitel Of Some Test Text","p":["Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh euismod tincidunt ut laoreet dolore magna aliquam erat volutpat."]},{"l":"Subsubtitel Of Some Test Text","p":["agfafga afadad#test#test2#test asas ok","Backing image download, sync, and deletion in a disk","Component","Controller/replica instance lifecycle management","Core controller/replica logic","Description","First","GitHub repo","Longhorn Backing Image Manager","Longhorn Engine","Longhorn Instance Manager","Longhorn Manager","Longhorn orchestration, includes CSI driver for Kubernetes","Longhorn Share Manager","Longhorn UI","longhorn/backing-image-manager","longhorn/longhorn-engine","longhorn/longhorn-instance-manager","longhorn/longhorn-manager","longhorn/longhorn-share-manager","longhorn/longhorn-ui","Nam liber tempor cum soluta nobis eleifend option congue nihil imperdiet doming id quod mazim placerat facer","NFS provisioner that exposes Longhorn volumes as ReadWriteMany volumes","Table","The Longhorn dashboard","This is a Description","Ut wisi enim ad minim veniam, quis nostrud exerci tation ullamcorper suscipit lobortis nisl ut aliquip ex ea commodo consequat. Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi.","What it does"]},{"l":"test","p":["(C) Robin Hermann 2021 Copyright"]}],[{"l":"Helm","p":["Helm helps you manage Kubernetes applications â€” Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.","Get Helm Default Values (example for project \"prometheus-community/kube-prometheus-stack\"): helm show values prometheus-community/kube-prometheus-stack","Testing Helm Chart (only local) helm template RELEASE_NAME .\\HELM-CHART_ROOT_LOCATION","Upgrade & Installing Helm Chart helm upgrade --install RELEASE_NAME -n NAMESPACE . \\HELM-CHART_ROOT_LOCATION","List Helm release helm list --namespace test-ns1","Uninstall Helm release helm uninstall RELEASE_NAME -n NAMESPACE"]},{"l":"Accessing a JSON-File inside Templates","p":["The function to accessing a JSON-File inside a template (yaml) is .Files.Get and with indent you define how many withspaces helm uses.","Important: You have to write {{ .Files.Get...}} at the very beginning of the line!","Example:"]},{"i":"range-foreach","l":"Range (foreach)","p":["The range function is like a foreach function inside your Helm templates. In the following example we create a DaemonSet foreach Pre-Pull image defined in the Helm values values.yaml.","Helm Template (daemonset.yaml)","Helm Values (values.yaml)"]},{"i":"use-a-helm-value-outside-the-range","l":"Use a Helm Value outside the \"range\"","p":["To use a Helm value outside the range for example from the root values you have to use \"$\" before the values path like this:{{ $.Values.YOUR_VALUENAME_AT_ROOT_PATH }}"]},{"l":"Troubleshooting"},{"i":"error-upgrade-failed-rendered-manifests-contain-a-resource-that-already-exists","l":"Error: UPGRADE FAILED: rendered manifests contain a resource that already exists","p":["Error","Cause This error occurs because you already have manually deployed a resource with the same name (in this example a \"ConfigMap\") and you are trying to redeploy this with Helm again. Helm cannot override the ConfigMap because it was not deployed with Helm.","Solution Remove the resource, in this example the ConfigMap, and redeploy with Helm."]}],[{"l":"Kubectl","p":["The Kubernetes command-line tool, Kubectl allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs."]},{"l":"Basic Commands","p":["Get configured contexts: kubectl config get-contexts Switch context: kubectl config use-context NAME Get pods: kubectl get pods -n NAMESPACE Kill pod: kubectl delete pod -n NAMESPACE POD_NAME --force --grace-period=0 Get more details (for example of a pod): kubectl get pod -n <POD_NAME> -o wide Describe pod: kubectl describe pod -n <POD_NAME> Get persistent volumes: kubectl get pvc Get pods filtered on label: kubectl get pods --all-namespaces -l app=nginx Open container shell: kubectl exec -n --stdin --tty <POD_NAME> -- /bin/sh Sort by (for example pod names): kubectl get pods --all-namespaces --sort-by=.metadata.name Sort by (for example persistent volumes): kubectl get pods --all-namespaces --sort-by=.spec.capacity.storage Sort by (for example events): kubectl get events --sort-by=.metadata.creationTimestamp Get all supported resource types: kubectl api-resources Create file and apply (secret example) Because kubectl create can only be run once, you can output the secret as YAML and pipe this to kubectl apply. kubectl create secret generic SECRET_NAME -n NAMESPACE --from-literal=USERNAME_VARIABLE=USERNAME --from-literal=PASSWORD_VARIABLE=PASSWORD --dry-run=client -o yaml | kubectl apply -f -","-> More in the Cheatsheet"]},{"l":"Namespaces","p":["Get all namespaces: kubectl get namespace Create namespace: kubectl create namespace NAMESPACE_NAME","Create namespace (example YAML):","-> Apply namespace: kubectl apply -f .\\namespace.yml"]},{"l":"Examples"},{"l":"Deployment of a basic Nginx Server","p":["deployment.yaml:","-> Apply deployment: kubectl apply -f .\\deployment.yml -n NAMESPACE","service.yaml:","-> Apply service: kubectl apply -f .\\service.yml -n NAMESPACE","ingress.yaml:","-> Apply ingress: kubectl apply -f .\\ingress.yaml -n NAMESPACE"]},{"i":"windows-autocompletion","l":"Windows: Autocompletion","p":["Add Kubectl autocompletion to load on each PowerShell session:"]},{"l":"Troubleshooting Kubernetes"},{"i":"kubectl-get-containers-resource-configuration-find-containers-without-configured-resources","l":"Kubectl: Get Containers Resource Configuration (find Containers without configured Resources)","p":["JSONPATH kubectl get pods -n NAMESPACE -o jsonpath={range .items[*]}{.metadata.name}{'\\t'}{.spec.containers[*].resources}{'\\n'}{end}","Custom Columns kubectl get pods -n grafana --output=custom-columns=POD:.metadata.name,CONTAINERS:.spec.containers[*].name,REQUESTS:.spec.containers[*].resources.requests,LIMITS:.spec.containers[*].resources.limits"]}],[{"l":"Kubernetes","p":["Rancher Kubernetes is using Docker as container runtime/engine.","Kubernetes internal communication): KUBERNETES_SERVICE_NAME:APPLICATION_PORT","Kubernetes internal communication (pods in different namespaces): KUBERNETES_SERVICE_NAME.KUBERNETES_NAMESPACE:APPLICATION_PORT"]},{"l":"Resources"},{"l":"Secrets","p":["A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code."]},{"l":"Creating Secrets","p":["Kubectl (example for USERNAME and PASSWORD)","YAML","Useful article"]},{"l":"Resource Management","p":["By default, containers run with unbounded compute resources on a Kubernetes cluster. With resource quotas, cluster administrators can restrict resource consumption and creation on a namespace basis. Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace's resource quota or by its limits.-> Basic Resource Management"]},{"i":"requests--limits-containers","l":"Requests & Limits (Containers)","p":["* When you specify a Pod, you can optionally specify how much of each resource a container needs.","Requests: When you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to place the Pod on and reserves at least the request amount of that system resource specifically for that container to use.","Limits: When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.","Note: If a container specifies its own cpu/memory limit, but does not specify a cpu/memory request, Kubernetes automatically assigns a cpu/memory request that matches the limit."]},{"i":"example-request--limit","l":"Example: Request & Limit"},{"i":"compute-resource-quotas-namespace","l":"Compute Resource Quotas (Namespace)","p":["A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption (REQUESTS & LIMITS) per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be requested or limited by resources in that namespace."]},{"l":"Limit Ranges","p":["A LimitRange is a policy to constrain resource allocations (to Pods or Containers) in a namespace and can provides constraints that can:","Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.","Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.","Enforce a ratio between request and limit for a resource in a namespace.","Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime."]},{"l":"Example Limit Range"},{"l":"Known-Errors","p":["Error: Create Pod ... failed error: pods \"...\" is forbidden: failed quota: <YOUR_QUOTA>: must specify limits.cpu,limits.memory Solution: Add limits to your container deployment.","Error: Create Pod ... failed error: pods \"...\" is forbidden: exceeded quota: <YOUR_QUOTA>, requested: limits.cpu=1600m, used: limits.cpu=1, limited: limits.cpu=2500m Solution: In this example my quota is exceeded ( limited - (requested + used) = -100m). -> Increase your quota by at least 100m."]},{"l":"Vertical Pod Autoscaler","p":["Doku Vertical Pod Autoscaler #todo"]},{"l":"Persistent Volumes","p":["Benchmarking Kubernetes Storage Solutions"]},{"l":"Longhorn","p":["_Longhorn delivers simplified, easy to deploy and upgrade, 100% open source, cloud-native hyper-converged persistent block storage without the cost overhead of open core or proprietary alternatives._","Project source code is spread across a number of repos:| Component | What it does | GitHub repo ||:------------------------------ |:---------------------------------------------------------------------- |:-------------------------------------- || Longhorn Backing Image Manager | Backing image download, sync, and deletion in a disk | longhorn/backing-image-manager|| Longhorn Engine | Core controller/replica logic | longhorn/longhorn-engine|| Longhorn Instance Manager | Controller/replica instance lifecycle management | longhorn/longhorn-instance-manager|| Longhorn Manager | Longhorn orchestration, includes CSI driver for Kubernetes | longhorn/longhorn-manager|| Longhorn Share Manager | NFS provisioner that exposes Longhorn volumes as ReadWriteMany volumes | longhorn/longhorn-share-manager|| Longhorn UI | The Longhorn dashboard | longhorn/longhorn-ui|"]},{"l":"Creating Longhorn Persistent Volumes using Rancher","p":["Create Project (I named it Storage)","In Rancher GUI: Cluster Tools -> Longhorn -> Install (default settings)","* If you want:","changing storage reserve (Longhorn GUI -> Node -> Edit node and disks)*","changing the storage overprovisioning percentage to 200% (Longhorn GUI -> Setting -> General -> Stroage Over...)","Now you have a new StorageClass called longhorn and it is ready to use."]},{"l":"Performance","p":["Currently there is a big discussion about Longhorns performance, more details in the GitHub Issue.-> Maybe this will be better in the future"]},{"l":"Monitoring","p":["Monitoring Longhorn is simple. All you need is:","A running Prometheus instance","A Kubernetes ServiceMonitor for Longhorn","Example: ServiceMonitor for Longhorn Longhorn ServiceMonitor has a label selector app: longhorn-manager to select Longhorn backend service. Later on, the Prometheus CRD can include Longhorn ServiceMonitor so that the Prometheus server can discover all Longhorn manager pods and their endpoints. Deploy the ServiceMonitor in the same namespace as Prometheus.","IMPORTANT: Unfortunately in my case the Prometheus Operator has not automatically found the new created ServiceMonitor. So I had to disable serviceMonitorSelectorNilUsesHelmValues and podMonitorSelectorNilUsesHelmValues like this (more information's in the GitHub issue):","Longhorn also offers a Grafana Dashboard(ID: 13032) that can be imported."]},{"l":"Rook","p":["Rook is a open-source Cloud-Native storage for Kubernetes that is a production ready management for File, Block and Object Storage by using Ceph. There are other storage providers like Cassandra and NFS (Alpha). Rook turns distributed storage systems into self-managing, self-scaling, self-healing storage services. It automates the tasks of a storage administrator: deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management."]},{"l":"Piraeus","p":["https://itnext.io/state-of-persistent-storage-in-k8s-a-benchmark-77a96bb1ac29","Piraeus-Project Piraeus-Operator"]},{"i":"taints--tolerations","l":"Taints & Tolerations","p":["Taints and tolerations are a mechanism that allows you to ensure that pods are not placed on inappropriate nodes. Taints are added to nodes, while tolerations are defined in the pod specification. When you taint a node, it will repel all the pods except those that have a toleration for that taint. A node can have one or many taints associated with it. For example, most Kubernetes distributions will automatically taint the master nodes so that one of the pods that manages the control plane is scheduled onto them and not any other data plane pods deployed by users. This ensures that the master nodes are dedicated to run control plane pods.-> Useful Guide"]},{"l":"Taint Effects","p":["Effect","Description","NoSchedule","The Kubernetes scheduler will only allow scheduling pods that have tolerations for the tainted nodes.","PreferNoSchedule","The Kubernetes scheduler will try to avoid scheduling pods that donâ€™t have tolerations for the tainted nodes.","NoExecute","Kubernetes will evict the running pods from the nodes if the pods donâ€™t have tolerations for the tainted nodes."]},{"l":"Taint Operations","p":["The default value for operator is Equal. A toleration \"matches\" a taint if the keys are the same and the effects are the same, and:","the operator is Exists(in which case no value should be specified), or","the operator is Equal and the value s are equal.","NOTE: There are two special cases: An empty key with operator Exists matches all keys, values and effects which means this will tolerate everything. An empty effect matches all effects with key key1."]},{"l":"Commands","p":["Get taints of all nodes:"]},{"i":"example-deploy-pod-on-controlplaneetcd-nodes","l":"Example: Deploy Pod on Controlplane/etcd Nodes","p":["For example you want to monitor your Kubernetes cluster using Prometheus Node Exporter, but your Prometheus Node Exporter pods are only deployed on the worker nodes (w01, w02 & w03). This is because you forgot define tolerations to your pods specification. To add this toleration to your pods, first check your nodes taint configurations. The following is an example:","Now you know the taints and can configure the tolerations. Because I used the Prometheus-Operator Helm chart to deploy all Prometheus Node Exporter I had to add following in my Helm values:","-> Normally this tolerations are defined in spec.tolerations:"]},{"l":"Tolerating all Taints","p":["You can configure a pod to tolerate all taints by adding an operator: Exists toleration with no key and value parameters. Pods with this toleration are not removed from a node that has taints."]},{"l":"Default Tolerations on Namespace","p":["There is a feature called PodTolerationRestriction to define default tolerations on namespace level. Example for namespace annotations:"]},{"l":"Security"},{"l":"Network Policies","p":["Dokumenttation Network Policies #todo"]},{"l":"Kubernetes Tools"},{"i":"kubernetes-local-testing-purposes","l":"Kubernetes local (Testing purposes)","p":["Recommended: https://rancherdesktop.io/"]},{"l":"Kaniko","p":["With Kaniko you are able to build container images on Kubernetes without root or privileged mode."]},{"l":"Troubleshooting","p":["Kubectl: Get Containers Resource Configuration/ Prometheus: Find Containers without configured Resources"]},{"i":"prometheus-find-containers-without-configured-resources-limits","l":"Prometheus: Find Containers without configured Resources (limits)"}],[{"l":"Pre-Pull Image","p":["Dokumentation Daemonset #todo"]}],[{"l":"Rancher"},{"l":"Troubleshooting"},{"l":"Re-Register Kubernetes Node in Rancher","p":["Switch to root and run ( https://rancher.com/docs/rancher/v2.5/en/cluster-admin/cleaning-cluster-nodes/#docker-containers-images-and-volumes)"]}],[{"l":"Docker","p":["Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly."]},{"l":"Docker Basic Commands","p":["Pull Docker image: docker pull nginx","Start container detached (without switching the context to the container): docker run -tid nginx","Run something in a container (example /bin/bash): docker exec -ti CONTAINER_ID /bin/bash","Remove Container: docker rm -f CONTAINER_ID Remove Container Image: docker rmi IMAGE_ID","Container Port forwarding (<EXTERNAL_PORT>:<INTERNAL_PORT>): docker run -tid -p 8080:80 --name test_webserver nginx","Docker logs (watchdog): docker logs CONTAINER_ID --tail 100 -f"]},{"l":"Create Docker Container Image","p":["Checklist for Images","Update the image ( apt-get update apt-get upgrade)","Clean update cache ( apt-get autoremove -y apt-get clean apt-get autoclean)","Add User and Group and container must run as user instead of root (always with User-ID instead of User-Name)","Port greater then 1024 (ports less then 1024 are reserved for priviliged users)","Use as less layers (commands) as possible (this decrease build time of an image)"]},{"i":"dockerfile-example---base-image-patched","l":"Dockerfile Example - Base Image (patched)","p":["Build: sudo docker build -t updated_ubuntu:v0.1 ."]},{"i":"dockerfile-example---running-a-bash-script-and-after-die","l":"Dockerfile Example - Running a Bash Script and after Die","p":["Build: sudo docker build -t updated_ubuntu:v1.0 ."]},{"l":"Layers","p":["![[../zz/images/docker_layers.png]] Basically, a layer, or image layer is a change on an image, or an intermediate image. Every command you specify ( FROM, RUN, COPY, etc.) in your Dockerfile causes the previous image to change, thus creating a new layer. You can think of it as staging changes when you're using git: You add a file's change, then another one, then another one...","Important: Use as less layers (commands) as possible (this decrease build time of a image).","If you create a Docker image and in your build process you want to compile for example some code in Go. Don't use a Go Docker image or install Go in your image. Although there is nothing technically wrong with the above process, the final image and the resulting container are bloated with layers created while building/preparing the project artefact that are not necessary for the projectâ€™s runtime environment. Multi-stage builds allow you to separate the creation/preparation phases from the runtime environment. More information in the next chapter..."]},{"l":"Docker Multi-Stage Builds","p":["![[../zz/images/docker_multistage.png]] You can still have a single Dockerfile to define your complete build workflow. However, you can copy artefacts from one stage to another while discarding the data in layers you donâ€™t need.","Example: The ratelimiter binary is built inside the golang based environment and then used in an alpine environment.","More information's: How to Build Optimal Docker Images"]},{"l":"Troubleshooting"},{"l":"Exploring Docker Images","p":["Figure out what kind of shell is in there bash or sh... Inspect the image first: docker inspect name-of-container-or-image Look for entrypoint or cmd in the JSON return.","Then do: docker run --rm -it --entrypoint=/bin/bash name-of-image once inside do: ls -lsa or any other shell command like: cd ..","The -it stands for interactive... and TTY. The --rm stands for remove container after run.","If there are no common tools like ls or bash present and you have access to the Dockerfile simple add the common tool as a layer. example (alpine Linux):"]}],[{"l":"GitLab"},{"l":"Pipelines","p":["Dokumentation Pipelines #todo","Examples mit","Script,","needs triggers (child-pipelines),","include"]},{"l":"Dynamic Child Pipelines","p":["https://docs.gitlab.com/ee/ci/pipelines/parent_child_pipelines.html#dynamic-child-pipelines"]}],[{"l":"Ansible"},{"i":"playbook-disable-swap","l":"Playbook: Disable Swap"}],[{"l":"Linux","p":["VIM Copy line: yy + p Copy 3 lines: 3 + yy + p Undo: u Remove line: dd Remove character: x New line a& insert mode: o"]},{"l":"Storage","p":["List block devices: lsblk"]},{"l":"LVM","p":["Physical Volume = PV Volume Group = VG Logical Volume = LV","List volume group: vgs List physical volumes: pvs List logical volumes: lvs","Create Volume","Extend Volume (incl. Filesystem)","New Volume in VG with all free space"]},{"l":"SED"},{"i":"regex-using-only-the-captured-group","l":"Regex using only the captured group:"},{"l":"Explanation","p":["-> SED Cheatsheet"]}],[{"l":"YAML"},{"i":"bash-only-function-to-parse-yaml-using-sed--awk","l":"Bash-only Function to parse YAML using SED / AWK","p":["YAML test file (sample.yaml)","Using the function","Output","Source"]}],[{"l":"Grafana"},{"l":"Grafana Operator","p":["The Grafana Operator can provision and manage Grafana Instances, Dashboards, Datasources and notification channels. Based on the Operator-SDK.","The following Grafana resources are supported (CRDs):","Grafana","GrafanaDashboard","GrafanaDatasource","GrafanaNotificationChannel","NOTE: Unfortunately there aren't CRDs for AlertRule, AlertGroup, ContactPoint and NotificationPolicy right now. More in the GitHub Feature request."]}],[{"l":"Prometheus","p":["An open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach."]},{"l":"Prometheus Operator","p":["Following CRDs are available right now:","Prometheus, which defines a desired Prometheus deployment.","Alertmanager, which defines a desired Alertmanager deployment.","ThanosRuler, which defines a desired Thanos Ruler deployment.","ServiceMonitor, which declaratively specifies how groups of Kubernetes services should be monitored. The Operator automatically generates Prometheus scrape configuration based on the current state of the objects in the API server.","PodMonitor, which declaratively specifies how group of pods should be monitored. The Operator automatically generates Prometheus scrape configuration based on the current state of the objects in the API server.","Probe, which declaratively specifies how groups of ingresses or static targets should be monitored. The Operator automatically generates Prometheus scrape configuration based on the definition.","PrometheusRule, which defines a desired set of Prometheus alerting and/or recording rules. The Operator generates a rule file, which can be used by Prometheus instances.","AlertmanagerConfig, which declaratively specifies subsections of the Alertmanager configuration, allowing routing of alerts to custom receivers, and setting inhibit rules.","NOTE: Unfortunately there is no CRD for AlertManager silences right now ( GitHub Feature request)."]},{"l":"Queries"},{"l":"Troubleshooting Kubernetes"},{"l":"Find Containers without configured Resources","p":["sum by (namespace,pod)(count by (namespace,pod,container)(kube_pod_container_info{container!=,namespace=NAMESPACE}) unless sum by (namespace,pod,container)(kube_pod_container_resource_limits{resource=cpu}))","More information's: Kubernetes resource limits"]},{"l":"Alertmanager","p":["Unfortunately Google's chat webhooks are not compatible with Alertmanager's webhooks. But there is a community project called calert that makes this possible.","NOTE: If you use calert the Alertmanager's receiver have to be named as the Google Chat name."]}],[{"l":"Error"},{"i":"error-1","l":"Error"},{"l":"Cause"},{"l":"Solution","p":["More information's:"]}],[{"l":"AnotherFile","p":["![Contributors] contributors-url![Forks] forks-url![Stargazers] stars-url![Issues] issues-url![MIT License] license-url![LinkedIn] linkedin-url"]},{"l":"About The Project","p":["![Product Name Screen Shot]( https://example.com)","There are many great README templates available on GitHub; however, I didn't find one that really suited my needs so I created this enhanced one. I want to create a README template so amazing that it'll be the last one you ever need -- I think this is it.","Here's why:","Your time should be focused on creating something amazing. A project that solves a problem and helps others","You shouldn't be doing the same tasks over and over like creating a README from scratch","You should implement DRY principles to the rest of your life \uD83D\uDE04","Of course, no one template will serve all projects since your needs may be different. So I'll be adding more in the near future. You may also suggest changes by forking this repo and creating a pull request or opening an issue. Thanks to all the people have contributed to expanding this template!","Use the BLANK_README.md to get started."]},{"l":"Built With","p":["This section should list any major frameworks/libraries used to bootstrap your project. Leave any add-ons/plugins for the acknowledgements section. Here are a few examples.","Next.js","React.js","Vue.js","Angular","Svelte","Laravel","Bootstrap","JQuery"]},{"l":"Getting Started","p":["This is an example of how you may give instructions on setting up your project locally. To get a local copy up and running follow these simple example steps."]},{"l":"Prerequisites","p":["This is an example of how to list things you need to use the software and how to install them.","npm"]},{"l":"Installation","p":["Below is an example of how you can instruct your audience on installing and setting up your app. This template doesn't rely on any external dependencies or services.","Get a free API Key at https://example.com","Clone the repo","Install NPM packages","Enter your API in config.js"]},{"l":"Usage","p":["Use this space to show useful examples of how a project can be used. Additional screenshots, code examples and demos work well in this space. You may also link to more resources.","For more examples, please refer to the Documentation"]},{"l":"Roadmap","p":["Add Changelog","Add back to top links","Add Additional Templates w/ Examples","Add \"components\" document to easily copy & paste sections of the readme","Multi-language Support","Chinese","Spanish","See the open issues for a full list of proposed features (and known issues)."]},{"l":"Contributing","p":["Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.","If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag \"enhancement\". Don't forget to give the project a star! Thanks again!","Fork the Project","Create your Feature Branch ( git checkout -b feature/AmazingFeature)","Commit your Changes ( git commit -m 'Add some AmazingFeature')","Push to the Branch ( git push origin feature/AmazingFeature)","Open a Pull Request"]},{"l":"License","p":["Distributed under the MIT License. See LICENSE.txt for more information."]},{"l":"Contact","p":["Your Name - @your_twitter- email@example.com","Project Link: https://github.com/your_username/repo_name"]},{"l":"Acknowledgments","p":["Use this space to list resources you find helpful and would like to give credit to. I've included a few of my favorites to kick things off!","Choose an Open Source License","GitHub Emoji Cheat Sheet","Malven's Flexbox Cheatsheet","Malven's Grid Cheatsheet","Img Shields","GitHub Pages","Font Awesome","React Icons"]}],[{"l":"Test Link","p":["README"]},{"l":"Formatting","p":["Markdown .md pages are plain text documents with a simple human readable syntax that aims to make writing for the internet easier.","No special software is required to create an .md file. Any basic text editor will do. Just save the file with a .md file extension.","Please see markdownguide.org for a full demonstration of the formatting possibilities and best practices.","View the actual formatting.md file used to create this page."]},{"l":"Quick start","p":["The following sample demonstrates a very basic .md page sample with page title and one paragraph.","We can build on the above sample by adding more content and formatting, such as bold text, images, and lists.","At a very basic level, to create a new page for your Retype project, do the following:","Make a readme.md file","Add a # title","Start writing"]},{"l":"Home page","p":["Ideally, your project will include a default file ( readme.md, index.md, or default.md) within the root of the project. If a default file is present within the root folder, Retype will use that page as your home page. Clicking on the top-left logo or title will navigate to the home page.","Those default files can also be placed inside of any folder within the project. Given the following folder and file structure, where Guides is a folder...","...Retype will create three pages in your website and the pages will be available at the following locations:","/","/guides/","/guides/getting-started/","If your home page is empty or blank, double check that you have a default page in the root of your project folder. The default file can be named readme.md, index.md, or default.md."]},{"l":"Components","p":["In addition to the standard Markdown options, Retype includes many custom components so you can easily add extra \uD83D\uDC8E flair \uD83D\uDC8E to your document.","The most commonly used Retype components include Alert and Tab:"]},{"l":"Alert","p":["This is an Alert"]},{"l":"Tab","p":["This is Tab 1","This is another Tab","See all components"]}],[{"l":"Kubernetes","p":["Rancher Kubernetes is using Docker as container runtime/engine.","Kubernetes internal communication): KUBERNETES_SERVICE_NAME:APPLICATION_PORT","Kubernetes internal communication (pods in different namespaces): KUBERNETES_SERVICE_NAME.KUBERNETES_NAMESPACE:APPLICATION_PORT"]},{"l":"Resources"},{"l":"Secrets","p":["A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code."]},{"l":"Creating Secrets","p":["Kubectl (example for USERNAME and PASSWORD)","YAML","Useful article"]},{"l":"Resource Management","p":["By default, containers run with unbounded compute resources on a Kubernetes cluster. With resource quotas, cluster administrators can restrict resource consumption and creation on a namespace basis. Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace's resource quota or by its limits.-> Basic Resource Management"]},{"i":"requests--limits-containers","l":"Requests & Limits (Containers)","p":["* When you specify a Pod, you can optionally specify how much of each resource a container needs.","Requests: When you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to place the Pod on and reserves at least the request amount of that system resource specifically for that container to use.","Limits: When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.","Note: If a container specifies its own cpu/memory limit, but does not specify a cpu/memory request, Kubernetes automatically assigns a cpu/memory request that matches the limit."]},{"i":"example-request--limit","l":"Example: Request & Limit"},{"i":"compute-resource-quotas-namespace","l":"Compute Resource Quotas (Namespace)","p":["A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption (REQUESTS & LIMITS) per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be requested or limited by resources in that namespace."]},{"l":"Limit Ranges","p":["A LimitRange is a policy to constrain resource allocations (to Pods or Containers) in a namespace and can provides constraints that can:","Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.","Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.","Enforce a ratio between request and limit for a resource in a namespace.","Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime."]},{"l":"Example Limit Range"},{"l":"Known-Errors","p":["Error: Create Pod ... failed error: pods \"...\" is forbidden: failed quota: <YOUR_QUOTA>: must specify limits.cpu,limits.memory Solution: Add limits to your container deployment.","Error: Create Pod ... failed error: pods \"...\" is forbidden: exceeded quota: <YOUR_QUOTA>, requested: limits.cpu=1600m, used: limits.cpu=1, limited: limits.cpu=2500m Solution: In this example my quota is exceeded ( limited - (requested + used) = -100m). -> Increase your quota by at least 100m."]},{"l":"Vertical Pod Autoscaler","p":["Doku Vertical Pod Autoscaler #todo"]},{"l":"Persistent Volumes","p":["Benchmarking Kubernetes Storage Solutions"]},{"l":"Longhorn","p":["_Longhorn delivers simplified, easy to deploy and upgrade, 100% open source, cloud-native hyper-converged persistent block storage without the cost overhead of open core or proprietary alternatives._","Project source code is spread across a number of repos:| Component | What it does | GitHub repo ||:------------------------------ |:---------------------------------------------------------------------- |:-------------------------------------- || Longhorn Backing Image Manager | Backing image download, sync, and deletion in a disk | longhorn/backing-image-manager|| Longhorn Engine | Core controller/replica logic | longhorn/longhorn-engine|| Longhorn Instance Manager | Controller/replica instance lifecycle management | longhorn/longhorn-instance-manager|| Longhorn Manager | Longhorn orchestration, includes CSI driver for Kubernetes | longhorn/longhorn-manager|| Longhorn Share Manager | NFS provisioner that exposes Longhorn volumes as ReadWriteMany volumes | longhorn/longhorn-share-manager|| Longhorn UI | The Longhorn dashboard | longhorn/longhorn-ui|"]},{"l":"Creating Longhorn Persistent Volumes using Rancher","p":["Create Project (I named it Storage)","In Rancher GUI: Cluster Tools -> Longhorn -> Install (default settings)","* If you want:","changing storage reserve (Longhorn GUI -> Node -> Edit node and disks)*","changing the storage overprovisioning percentage to 200% (Longhorn GUI -> Setting -> General -> Stroage Over...)","Now you have a new StorageClass called longhorn and it is ready to use."]},{"l":"Performance","p":["Currently there is a big discussion about Longhorns performance, more details in the GitHub Issue.-> Maybe this will be better in the future"]},{"l":"Monitoring","p":["Monitoring Longhorn is simple. All you need is:","A running Prometheus instance","A Kubernetes ServiceMonitor for Longhorn","Example: ServiceMonitor for Longhorn Longhorn ServiceMonitor has a label selector app: longhorn-manager to select Longhorn backend service. Later on, the Prometheus CRD can include Longhorn ServiceMonitor so that the Prometheus server can discover all Longhorn manager pods and their endpoints. Deploy the ServiceMonitor in the same namespace as Prometheus.","IMPORTANT: Unfortunately in my case the Prometheus Operator has not automatically found the new created ServiceMonitor. So I had to disable serviceMonitorSelectorNilUsesHelmValues and podMonitorSelectorNilUsesHelmValues like this (more information's in the GitHub issue):","Longhorn also offers a Grafana Dashboard(ID: 13032) that can be imported."]},{"l":"Rook","p":["Rook is a open-source Cloud-Native storage for Kubernetes that is a production ready management for File, Block and Object Storage by using Ceph. There are other storage providers like Cassandra and NFS (Alpha). Rook turns distributed storage systems into self-managing, self-scaling, self-healing storage services. It automates the tasks of a storage administrator: deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management."]},{"l":"Piraeus","p":["https://itnext.io/state-of-persistent-storage-in-k8s-a-benchmark-77a96bb1ac29","Piraeus-Project Piraeus-Operator"]},{"i":"taints--tolerations","l":"Taints & Tolerations","p":["Taints and tolerations are a mechanism that allows you to ensure that pods are not placed on inappropriate nodes. Taints are added to nodes, while tolerations are defined in the pod specification. When you taint a node, it will repel all the pods except those that have a toleration for that taint. A node can have one or many taints associated with it. For example, most Kubernetes distributions will automatically taint the master nodes so that one of the pods that manages the control plane is scheduled onto them and not any other data plane pods deployed by users. This ensures that the master nodes are dedicated to run control plane pods.-> Useful Guide"]},{"l":"Taint Effects","p":["Effect","Description","NoSchedule","The Kubernetes scheduler will only allow scheduling pods that have tolerations for the tainted nodes.","PreferNoSchedule","The Kubernetes scheduler will try to avoid scheduling pods that donâ€™t have tolerations for the tainted nodes.","NoExecute","Kubernetes will evict the running pods from the nodes if the pods donâ€™t have tolerations for the tainted nodes."]},{"l":"Taint Operations","p":["The default value for operator is Equal. A toleration \"matches\" a taint if the keys are the same and the effects are the same, and:","the operator is Exists(in which case no value should be specified), or","the operator is Equal and the value s are equal.","NOTE: There are two special cases: An empty key with operator Exists matches all keys, values and effects which means this will tolerate everything. An empty effect matches all effects with key key1."]},{"l":"Commands","p":["Get taints of all nodes:"]},{"i":"example-deploy-pod-on-controlplaneetcd-nodes","l":"Example: Deploy Pod on Controlplane/etcd Nodes","p":["For example you want to monitor your Kubernetes cluster using Prometheus Node Exporter, but your Prometheus Node Exporter pods are only deployed on the worker nodes (w01, w02 & w03). This is because you forgot define tolerations to your pods specification. To add this toleration to your pods, first check your nodes taint configurations. The following is an example:","Now you know the taints and can configure the tolerations. Because I used the Prometheus-Operator Helm chart to deploy all Prometheus Node Exporter I had to add following in my Helm values:","-> Normally this tolerations are defined in spec.tolerations:"]},{"l":"Tolerating all Taints","p":["You can configure a pod to tolerate all taints by adding an operator: Exists toleration with no key and value parameters. Pods with this toleration are not removed from a node that has taints."]},{"l":"Default Tolerations on Namespace","p":["There is a feature called PodTolerationRestriction to define default tolerations on namespace level. Example for namespace annotations:"]},{"l":"Security"},{"l":"Network Policies","p":["Dokumenttation Network Policies #todo"]},{"l":"Kubernetes Tools"},{"i":"kubernetes-local-testing-purposes","l":"Kubernetes local (Testing purposes)","p":["Recommended: https://rancherdesktop.io/"]},{"l":"Kaniko","p":["With Kaniko you are able to build container images on Kubernetes without root or privileged mode."]},{"l":"Troubleshooting","p":["Kubectl: Get Containers Resource Configuration/ Prometheus: Find Containers without configured Resources"]},{"i":"prometheus-find-containers-without-configured-resources-limits","l":"Prometheus: Find Containers without configured Resources (limits)"}],[{"l":"Test","p":["1","2","3","asas","asdas","center","edited direct in GitHub","efef","left","README","right","sdsds","sdsdsd","TestFile","TestFile in Folder1","testfilewithpicturefromanother"]}],[{"l":"TestTitle"},{"l":"Dillinger"},{"i":"the-last-markdown-editor-ever","l":"The Last Markdown Editor, Ever","p":["N|Solid","sdsdsd","Build Status","sfsf","Dillinger is a cloud-enabled, mobile-ready, offline-storage compatible, AngularJS-powered HTML5 Markdown editor.","Type some Markdown on the left","See HTML in the right","âœ¨Magic âœ¨"]},{"l":"Features","p":["Import a HTML file and watch it magically convert to Markdown","Drag and drop images (requires your Dropbox account be linked)","Import and save files from GitHub, Dropbox, Google Drive and One Drive","Drag and drop markdown and HTML files into Dillinger","Export documents as Markdown, HTML and PDF","Markdown is a lightweight markup language based on the formatting conventions that people naturally use in email. As John Gruber writes on the Markdown site","The overriding design goal for Markdown's formatting syntax is to make it as readable as possible. The idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it's been marked up with tags or formatting instructions.","This text you see here is * actually- written in Markdown! To get a feel for Markdown's syntax, type some text into the left window and watch the results in the right."]},{"l":"Tech","p":["Dillinger uses a number of open source projects to work properly:","AngularJS- HTML enhanced for web apps!","Ace Editor- awesome web-based text editor","markdown-it- Markdown parser done right. Fast and easy to extend.","Twitter Bootstrap- great UI boilerplate for modern web apps","node.js- evented I/O for the backend","Express- fast node.js network app framework @tjholowaychuk","Gulp- the streaming build system","Breakdance- HTML to Markdown converter","jQuery- duh","And of course Dillinger itself is open source with a public repository on GitHub."]},{"l":"Installation","p":["Dillinger requires Node.js v10+ to run.","Install the dependencies and devDependencies and start the server.","For production environments..."]},{"l":"Plugins","p":["Dillinger is currently extended with the following plugins. Instructions on how to use them in your own application are linked below.","Plugin","README","Dropbox","plugins/dropbox/README.md","GitHub","plugins/github/README.md","Google Drive","plugins/googledrive/README.md","OneDrive","plugins/onedrive/README.md","Medium","plugins/medium/README.md","Google Analytics","plugins/googleanalytics/README.md"]},{"l":"Development","p":["Want to contribute? Great!","Dillinger uses Gulp + Webpack for fast developing. Make a change in your file and instantaneously see your updates!","Open your favorite Terminal and run these commands.","First Tab:","Second Tab:","(optional) Third:"]},{"l":"Building for source","p":["For production release:","Generating pre-built zip archives for distribution:"]},{"l":"Docker","p":["Dillinger is very easy to install and deploy in a Docker container.","By default, the Docker will expose port 8080, so change this within the Dockerfile if necessary. When ready, simply use the Dockerfile to build the image.","This will create the dillinger image and pull in the necessary dependencies. Be sure to swap out ${package.json.version} with the actual version of Dillinger.","Once done, run the Docker image and map the port to whatever you wish on your host. In this example, we simply map port 8000 of the host to port 8080 of the Docker (or whatever port was exposed in the Dockerfile):","Note: --capt-add=SYS-ADMIN is required for PDF rendering.","Verify the deployment by navigating to your server address in your preferred browser."]},{"l":"License","p":["MIT","Free Software, Hell Yeah!"]},{"l":"Merged Title","p":["Some Text"]}],[{"l":"Title","p":["iamge","Some Changes"]}],[{"l":"Awesome Tools","p":["Everything-","ExtPart (Windows Server 2003)-","Folder2ISO-","Handbrake-","InstEd-","IOZone-","MalwareBytes-","mRemoteNG-","My short list about tools that I like.","OnShutDown-","PDF24-","PowerToys-","Process Explorer (SysInternals)-","QEMU-img-","Rufus-","RVTools-","ShareX-","Sublime Text-","TestDisk-","TSS (Microsoft TroubleShooting Script)-","VLC-","WMIMon-","XPerf & Performance Analyzer-"]}],[{"l":"RETYPE GITHUB INFOS","p":["Configure ReType Project","Create Workflow & Push","Create GitHUb Pages"]}],[{"l":"TestTitle"},{"l":"Dillinger"},{"i":"the-last-markdown-editor-ever","l":"The Last Markdown Editor, Ever","p":["N|Solid","Build Status","Test Picture","Dillinger is a cloud-enabled, mobile-ready, offline-storage compatible, AngularJS-powered HTML5 Markdown editor.","Type some Markdown on the left","See HTML in the right","âœ¨Magic âœ¨"]},{"l":"Features","p":["Import a HTML file and watch it magically convert to Markdown","Drag and drop images (requires your Dropbox account be linked)","Import and save files from GitHub, Dropbox, Google Drive and One Drive","Drag and drop markdown and HTML files into Dillinger","Export documents as Markdown, HTML and PDF","Markdown is a lightweight markup language based on the formatting conventions that people naturally use in email. As John Gruber writes on the Markdown site","The overriding design goal for Markdown's formatting syntax is to make it as readable as possible. The idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it's been marked up with tags or formatting instructions.","This text you see here is * actually- written in Markdown! To get a feel for Markdown's syntax, type some text into the left window and watch the results in the right."]},{"l":"Tech","p":["Dillinger uses a number of open source projects to work properly:","AngularJS- HTML enhanced for web apps!","Ace Editor- awesome web-based text editor","markdown-it- Markdown parser done right. Fast and easy to extend.","Twitter Bootstrap- great UI boilerplate for modern web apps","node.js- evented I/O for the backend","Express- fast node.js network app framework @tjholowaychuk","Gulp- the streaming build system","Breakdance- HTML to Markdown converter","jQuery- duh","And of course Dillinger itself is open source with a public repository on GitHub."]},{"l":"Installation","p":["Dillinger requires Node.js v10+ to run.","Install the dependencies and devDependencies and start the server.","For production environments..."]},{"l":"Plugins","p":["Dillinger is currently extended with the following plugins. Instructions on how to use them in your own application are linked below.","Plugin","README","Dropbox","plugins/dropbox/README.md","GitHub","plugins/github/README.md","Google Drive","plugins/googledrive/README.md","OneDrive","plugins/onedrive/README.md","Medium","plugins/medium/README.md","Google Analytics","plugins/googleanalytics/README.md"]},{"l":"Development","p":["Want to contribute? Great!","Dillinger uses Gulp + Webpack for fast developing. Make a change in your file and instantaneously see your updates!","Open your favorite Terminal and run these commands.","First Tab:","Second Tab:","(optional) Third:"]},{"l":"Building for source","p":["For production release:","Generating pre-built zip archives for distribution:"]},{"l":"Docker","p":["Dillinger is very easy to install and deploy in a Docker container.","By default, the Docker will expose port 8080, so change this within the Dockerfile if necessary. When ready, simply use the Dockerfile to build the image.","This will create the dillinger image and pull in the necessary dependencies. Be sure to swap out ${package.json.version} with the actual version of Dillinger.","Once done, run the Docker image and map the port to whatever you wish on your host. In this example, we simply map port 8000 of the host to port 8080 of the Docker (or whatever port was exposed in the Dockerfile):","Note: --capt-add=SYS-ADMIN is required for PDF rendering.","Verify the deployment by navigating to your server address in your preferred browser.","Alt Text"]},{"l":"License","p":["MIT","Free Software, Hell Yeah!"]}]]